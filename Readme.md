# ðŸ—„ï¸ Distributed Key-Value Store

A simple yet powerful distributed key-value store showcasing **leader election**, **consistent hashing**, **replication**, and **fault tolerance**.

## âœ… Features

### 1. **Leader Election**

- Cluster elects a leader automatically
- All nodes recognize the elected leader (e.g., `localhost:8001`)
- Leader status is shared across the cluster

### 2. **Consistent Hashing Distribution**

- Keys are distributed across nodes based on their hash values
- Example: `user1` is routed to `node2` (not the leader)

### 3. **Replication**

- Each key is stored on a primary + replica(s) for fault tolerance
- Example: `user1` stored on `node2` (primary) and `node3` (replica)
- Leader may not store the key at all (acts as coordinator only)

---

## ðŸš€ Usage

### Start the Cluster

Run nodes in separate terminals:

```bash
python src/node.py node1 localhost 8001 localhost:8002,localhost:8003
python src/node.py node2 localhost 8002 localhost:8001,localhost:8003
python src/node.py node3 localhost 8003 localhost:8001,localhost:8002
```

Start the CLI:

```bash
python src/cli.py
```

---

## ðŸ§ª Testing Scenarios

### Test 1: Add Keys and Check Distribution

```bash
kvstore> put user2 Bob
kvstore> put user3 Charlie
kvstore> put user4 David
kvstore> put user5 Eve
kvstore> status
```

ðŸ‘‰ Keys should be distributed across nodes with replication.

### Test 2: Retrieve Data

```bash
kvstore> get user1   # â†’ Alice (from node2 or replica)
kvstore> get user2   # â†’ Bob
kvstore> get user3   # â†’ Charlie
```

### Test 3: Leader Failover

```bash
kvstore> status      # check current leader
# Kill node1 (Ctrl+C in Terminal 1)
# Wait 10-15s
kvstore> status      # new leader elected
kvstore> put after_failover test_data
kvstore> get user1   # still works via replica
```

### Test 4: Node Recovery

```bash
# Restart the killed node
python src/node.py node1 localhost 8001 localhost:8002,localhost:8003

kvstore> status
# node1 rejoins as follower
```

---

## ðŸ”Ž Architecture Overview

1. **Consistent Hashing** â†’ Deterministic key-to-node mapping
2. **Replication** â†’ Data stored on primary + replicas
3. **Leader Election** â†’ One leader coordinates operations
4. **Load Distribution** â†’ Keys spread across cluster
5. **Fault Tolerance** â†’ Cluster survives node failures

---

## ðŸ§© Key Insights

- **Leader â‰  Data Store** â†’ Leader coordinates, not necessarily stores data
- **Hash-Based Routing** â†’ Same key always goes to same node
- **Automatic Replication** â†’ Built-in fault tolerance
- **Resilience** â†’ Operations continue despite failures

---

## ðŸ”® Next Steps

- Add more keys and observe distribution
- Kill/restart different nodes for failure testing
- Watch logs for deeper insight into cluster mechanics

---

âœ¨ This project demonstrates core distributed systems concepts:

- âœ… Consistent Hashing
- âœ… Replication
- âœ… Leader Election
- âœ… Fault Tolerance
- âœ… Load Distribution

---
